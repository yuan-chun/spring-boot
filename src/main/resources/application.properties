#只对格式(application-{profile}.properties)定义的配置文件起激活作用
spring.profiles.active=@profileActive@
#合并配置文件
#spring.profiles.include=dev

#log
#TRACE < DEBUG < INFO < WARN < ERROR < FATAL
spring.main.banner-mode=off
#logging.level.root=info
logging.level.org.apache.ibatis=debug
logging.level.com.yuanchun.dao=debug
logging.level.org.springframework.web=debug
#logging.file=springlog.log
#logging.pattern.console=%d{yyyy/MM/dd-HH:mm:ss} %clr([%thread]){green} %clr(%-5level){yellow} %clr(%logger){blue} : %msg%n
#logging.pattern.file=%d{yyyy/MM/dd-HH:mm} [%thread] %-5level %logger- %msg%n

#mysql
spring.datasource.driver-class-name=@db.driver@
spring.datasource.url=@db.url@
spring.datasource.username=@db.username@
spring.datasource.password=@db.password@
spring.datasource.type=@db.type@

#server.port
server.port=@server.port@


#spark jdbc
#spark.sparksql.jdbc.url=jdbc:hive2://jzhdp10:2181,jzhdp11:2181,jzhdp12:2181/clexe;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkserver2
spark.sparksql.jdbc.url=jdbc:hive2://jzhdp10:2181,jzhdp11:2181,jzhdp12:2181/clexe;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
#spark.sparksql.jdbc.url=jdbc:hive2://135.160.19.9:10013/clexe
spark.sparksql.jdbc.driver=org.apache.hive.jdbc.HiveDriver
spark.sparksql.jdbc.userName=hadoop
spark.sparksql.jdbc.password=hadoop
spark.sparksql.jdbc.retry=3
spark.isLoad=true

#mybatis
mybatis.type-aliases-package=com.yuanchun.entry
# 驼峰命名规范 如：数据库字段是  order_id 那么 实体字段就要写成 orderId,xx-mapper.xml中的resultMap的映射可以省略掉了
mybatis.configuration.map-underscore-to-camel-case=true
#xml放在java下
mybatis.mapper-locations=classpath:com/yuanchun/mapper/*.xml
#xml放在resoources下
#mybatis.mapper-Locations=classpath:mapper/*.xml

########## 通用Mapper ##########
# 主键自增回写方法,默认值MYSQL,详细说明请看文档
mapper.identity=MYSQL
mapper.mappers=tk.mybatis.mapper.common.BaseMapper
# 设置 insert 和 update 中，是否判断字符串类型!=''
mapper.not-empty=true
# 枚举按简单类型处理
mapper.enum-as-simple-type=true

#mybatis-pagehelp
pagehelper.Dialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params=count=countSql

#文件上传配置
web.upload-path=/nas/data/upload
springboot.mvc.static-path-pattern=/**
spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/static,classpath:/resources/,file:{web.upload-path}
application.imageUploadPath=/nas1/data/upload/

#============== kafka ===================
kafka.consumer.zookeeper.connect=192.168.91.11:2181,192.168.91.12:2181,192.168.91.13:2181
kafka.consumer.servers=192.168.91.11:9091,192.168.91.12:9092,192.168.91.13:9093
#是否自动提交
kafka.consumer.enable.auto.commit=false
#自动提交时间间隔
kafka.consumer.auto.commit.interval=100
kafka.consumer.session.timeout=6000
#如果新的groupid消费的offset已经保存在__consumer__offsets这个Topic下，则并不会从开始消费所有数据，而是从最新的offset开始消费.
#对于已经存在的topic ,新增对其监听消费者组时的读取策略，earliest/latest，新增后这个offset会保存到kafka服务端的topic:__consumer__offsets中
# 而是保存在Kafka的一个内部Topic中__consumer_offsets，该Topic默认有50个Partition，每个Partition有3个副本，
# 分区数量由参数offset.topic.num.partition设置。通过groupId的哈希值和该参数取模的方式来确定某个消费者组已消费的offset保存到__consumer_offsets主题的哪个分区中。
# --from-beginning  和 --offset earliest/latest/offsetvalue --partition 0 不能同时使用，后者会真实修改offset值
#对于是新增没有组的消费者，也可以指定读取策略，earliest/latest/offsetvalue   --from-beginning（一直有效，因为没有groupid不会__consumer__offsets中保存）  --offset earliest/offsetvalue --partition 0
kafka.consumer.auto.offset.reset=latest
kafka.consumer.topic=topic-1
kafka.consumer.group.id=test
kafka.consumer.concurrency=10

kafka.producer.servers=192.168.91.11:9091,192.168.91.12:9092,192.168.91.13:9093
kafka.producer.retries=0
kafka.producer.batch.size=4096
kafka.producer.linger=1
kafka.producer.buffer.memory=40960
